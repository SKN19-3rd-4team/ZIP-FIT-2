{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "# ëª¨ë“  database ê´€ë ¨ ëª¨ë“ˆ ì œê±°\n",
    "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('database')]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "\n",
    "# ë‹¤ì‹œ import\n",
    "import database\n",
    "\n",
    "from database.db_handler import DataBaseHandler\n",
    "from database.repository import AnncLhRepository\n",
    "lh_repo = AnncLhRepository()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911cf0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_handler = DataBaseHandler()\n",
    "# db_handler.set_default_tables(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94617e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb74ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================================\n",
    "# 1. í™˜ê²½ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜\n",
    "# ==================================\n",
    "\n",
    "# ğŸš© í•„í„° ê¸°ì¤€: ê²Œì‹œì¼ >= 2024-11-01\n",
    "START_DATE_FILTER = datetime(2024, 11, 1)\n",
    "START_DATE_REQUEST = START_DATE_FILTER.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ (ì¡°íšŒ ì¢…ë£Œì¼)\n",
    "TODAY_DATE_REQUEST = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "BASE_URL = \"https://apply.lh.or.kr/lhapply/apply/wt/wrtanc/selectWrtancList.do\"\n",
    "DETAIL_URL_BASE = \"https://apply.lh.or.kr/lhapply/apply/wt/wrtanc/selectWrtancInfo.do?mi=1026\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://apply.lh.or.kr/\"\n",
    "}\n",
    "\n",
    "# Payload í…œí”Œë¦¿\n",
    "FORM_DATA_TEMPLATE = {\n",
    "    'currPage': 1,\n",
    "    'panId': '',\n",
    "    'cnpCd': '',\n",
    "    'prevListCo': 50,\n",
    "    'srchAisTpCd': '',\n",
    "    'panSs': '',\n",
    "    'startDt': START_DATE_REQUEST,\n",
    "    'endDt': TODAY_DATE_REQUEST,\n",
    "    'aisTpCd': '',\n",
    "    'uppAisTpCd': '',\n",
    "    'mi': '',\n",
    "    'srchUppAisTpCd': '',\n",
    "    'srchY': 'N',\n",
    "    'srchFilter': 'N',\n",
    "    'csCd': '',\n",
    "    'CNP_CD': '',\n",
    "    'ccrCnntSysDsCd': '',\n",
    "    'xssChk': 'N',\n",
    "    'panEdDt': TODAY_DATE_REQUEST.replace('-', ''),\n",
    "    'listCo': 100,\n",
    "    'panNm': '',\n",
    "    'indVal': 'N',\n",
    "    'maxSn': 50,\n",
    "    'mvinQf': '',\n",
    "    'netbgn': '',\n",
    "    'viewType': '',\n",
    "    'panStDt': '',\n",
    "    'minSn': 0,\n",
    "    'schTy': '0',\n",
    "    'page': 1,\n",
    "}\n",
    "\n",
    "# ğŸš© ì„ëŒ€/ë¶„ì–‘ ìœ í˜• ëª©ë¡\n",
    "LEASE_TYPES = [\n",
    "    \"í†µí•©ê³µê³µì„ëŒ€\", \"í†µí•©ê³µê³µì„ëŒ€(ì‹ í˜¼í¬ë§)\", \"êµ­ë¯¼ì„ëŒ€\", \"ê³µê³µì„ëŒ€\", \"ì˜êµ¬ì„ëŒ€\", \"í–‰ë³µì£¼íƒ\",\n",
    "    \"í–‰ë³µì£¼íƒ(ì‹ í˜¼í¬ë§)\", \"ì¥ê¸°ì „ì„¸\", \"ì‹ ì¶•ë‹¤ì„¸ëŒ€ë§¤ì…ì„ëŒ€\", \"ê°€ì •ì–´ë¦°ì´ì§‘\", \"ë§¤ì…ì„ëŒ€\",\n",
    "    \"ì „ì„¸ì„ëŒ€\", \"ì§‘ì£¼ì¸ì„ëŒ€\", \"6ë…„ ê³µê³µì„ëŒ€ì£¼íƒ\"\n",
    "]\n",
    "\n",
    "SALE_TYPES = [\n",
    "    \"ë¶„ì–‘ì£¼íƒ\", \"ê³µê³µë¶„ì–‘(ì‹ í˜¼í¬ë§)\"\n",
    "]\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# 2. í¬ë¡¤ë§ í•µì‹¬ í•¨ìˆ˜\n",
    "# ==================================\n",
    "\n",
    "def crawl_lh_notices_all_data():\n",
    "    \"\"\"LH í™ˆí˜ì´ì§€ì—ì„œ 2024-11-01 ì´í›„ì˜ ëª¨ë“  ê³µê³  ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.\"\"\"\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    list_count = FORM_DATA_TEMPLATE['listCo']\n",
    "\n",
    "    this_title = \"\"\n",
    "\n",
    "    print(\"ğŸš€ LH ê³µê³  ë°ì´í„° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (POST ë°©ì‹)...\")\n",
    "    print(f\"**í•„í„° ê¸°ì¤€: [ëª¨ë“  ìœ í˜•] + ê²Œì‹œì¼ {FORM_DATA_TEMPLATE['startDt']} ì´í›„ ë°ì´í„° ìˆ˜ì§‘.**\")\n",
    "\n",
    "    while True:\n",
    "        form_data = FORM_DATA_TEMPLATE.copy()\n",
    "        form_data['currPage'] = str(page)\n",
    "        form_data['page'] = str(page)\n",
    "        form_data['minSn'] = str((page - 1) * list_count)\n",
    "        form_data['maxSn'] = str(page * list_count)\n",
    "        form_data['prevListCo'] = str(list_count)\n",
    "\n",
    "        print(f\"\\nğŸ“„ Crawling page {page} ({form_data['minSn']} to {form_data['maxSn']})...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.post(BASE_URL, data=form_data, headers=HEADERS, timeout=15)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            rows = soup.select(\"div.bbs_ListA table tbody tr\")\n",
    "            raw_data_count = len(rows)\n",
    "\n",
    "            if raw_data_count == 0:\n",
    "                print(\"ğŸ ë°ì´í„° ì—†ìŒ. ëª©ë¡ì˜ ëì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.\")\n",
    "                break\n",
    "\n",
    "            new_data_count = 0\n",
    "            stop_crawling = False\n",
    "\n",
    "            for row in rows:\n",
    "                try:\n",
    "                    cols = [c.get_text(strip=True) for c in row.select(\"td\")]\n",
    "                    if len(cols) < 9:\n",
    "                        continue\n",
    "\n",
    "                    status = cols[7]\n",
    "                    post_date_str = cols[5]\n",
    "\n",
    "                    try:\n",
    "                        post_date = datetime.strptime(post_date_str, '%Y.%m.%d')\n",
    "                        if post_date < START_DATE_FILTER:\n",
    "                            print(f\"ğŸš© ê²Œì‹œì¼ {post_date_str} ë°ì´í„°ê°€ í•„í„° ê¸°ì¤€({START_DATE_REQUEST})ë³´ë‹¤ ì´ì „ì…ë‹ˆë‹¤. ì¶”ê°€ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
    "                            stop_crawling = True\n",
    "                            break\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                    title_anchor = row.select_one(\"td:nth-child(3) a\")\n",
    "                    em_tag = title_anchor.find('em')\n",
    "                    if em_tag:\n",
    "                        em_tag.extract()\n",
    "                    title = title_anchor.get_text(strip=True).replace('\\n', ' ')\n",
    "\n",
    "                    this_title = title\n",
    "\n",
    "                    \"\"\"HTML ìš”ì†Œì—ì„œ ìƒì„¸ í˜ì´ì§€ ì¡°íšŒì— í•„ìš”í•œ data ì†ì„±ì„ ì¶”ì¶œí•˜ì—¬ URLì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "                    data_panId = title_anchor.get('data-id1', '')\n",
    "                    data_ccr = title_anchor.get('data-id2', '')\n",
    "                    data_upp = title_anchor.get('data-id3', '')\n",
    "                    data_ais = title_anchor.get('data-id4', '')\n",
    "\n",
    "                    if all([data_panId, data_ccr, data_upp, data_ais]):\n",
    "                        detail_url = (\n",
    "                            f\"{DETAIL_URL_BASE}&\"\n",
    "                            f\"panId={data_panId}&\"\n",
    "                            f\"ccrCnntSysDsCd={data_ccr}&\"\n",
    "                            f\"uppAisTpCd={data_upp}&\"\n",
    "                            f\"aisTpCd={data_ais}\"\n",
    "                        )\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    # íŒŒì¼ ì •ë³´ ì—†ëŠ” ê²½ìš° ë¦¬í„´\n",
    "                    file_down_obj = row.select_one(\".listFileDown\")\n",
    "\n",
    "                    if file_down_obj:\n",
    "                        upp_ais_tp_cd = file_down_obj.get('data-id1', '')\n",
    "                        ais_tp_cd = file_down_obj.get('data-id2', '')\n",
    "                        ccr_cnnt_sys_ds_cd = file_down_obj.get('data-id3', '')\n",
    "                        ls_sst = file_down_obj.get('data-id4', '')\n",
    "                        pan_id = file_down_obj.get('data-id5', '')\n",
    "                    else:\n",
    "                        upp_ais_tp_cd = title_anchor.get('data-id3', '')\n",
    "                        ais_tp_cd = title_anchor.get('data-id4', '')\n",
    "                        ccr_cnnt_sys_ds_cd = title_anchor.get('data-id2', '')\n",
    "                        ls_sst = \"\"\n",
    "                        pan_id = title_anchor.get('data-id1', '')\n",
    "\n",
    "\n",
    "                    # ê³µê³  ìœ í˜• íŒë³„\n",
    "                    annc_type_simple = 'ê¸°íƒ€'\n",
    "\n",
    "                    if cols[1] in LEASE_TYPES:\n",
    "                        annc_type_simple = 'ì„ëŒ€'\n",
    "                    elif cols[1] in SALE_TYPES:\n",
    "                        annc_type_simple = 'ë¶„ì–‘'\n",
    "\n",
    "\n",
    "                    all_data.append(\n",
    "                        {\n",
    "                            'annc_url': detail_url, # ê³µê³  URL\n",
    "                            'annc_type': annc_type_simple, # ê³µê³  ìœ í˜•\n",
    "                            'annc_dtl_type': cols[1], # ê³µê³  ìœ í˜• ìƒì„¸\n",
    "                            'annc_region': cols[3], # ì§€ì—­\n",
    "                            'annc_pblsh_dt': post_date_str, # ê²Œì‹œì¼\n",
    "                            'annc_deadline_dt': cols[6], # ë§ˆê°ì¼\n",
    "                            'annc_status': status, # ê³µê³  ìƒíƒœ\n",
    "                            'lh_pan_id': pan_id, # ê³µê³  ì‹ë³„ ID\n",
    "                            'lh_ais_tp_cd': ais_tp_cd, # ê³µê³  ìœ í˜• ì½”ë“œ\n",
    "                            'lh_upp_ais_tp_cd': upp_ais_tp_cd, # ìƒìœ„ ê³µê³  ìœ í˜• ì½”ë“œ\n",
    "                            'lh_ccr_cnnt_sys_ds_cd': ccr_cnnt_sys_ds_cd, # ì—°ê³„ ì‹œìŠ¤í…œ êµ¬ë¶„ ì½”ë“œ\n",
    "                            'lh_ls_sst': ls_sst, # ëª©ë¡ ìƒì˜ ìƒíƒœ/ìˆœì„œ\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "\n",
    "                    new_data_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"ğŸš¨ ì˜¤ë¥˜: {e} - {this_title}\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"âœ… í˜ì´ì§€ {page} ë¡œë“œ ì„±ê³µ. {len(rows)}ê°œ ì¤‘ {new_data_count}ê°œ ë°ì´í„° ì¶”ì¶œ.\")\n",
    "\n",
    "            if stop_crawling or raw_data_count < list_count:\n",
    "                print(\"ğŸ í¬ë¡¤ë§ ì¢…ë£Œ ì¡°ê±´ ì¶©ì¡±. ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ.\")\n",
    "                break\n",
    "\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"ğŸš¨ ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” HTTP ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"ğŸš¨ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            break\n",
    "\n",
    "    if all_data:\n",
    "        print(f\"\\nâœ¨ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ. ì´ {len(all_data)}ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        # return pd.DataFrame(all_data)\n",
    "        return all_data\n",
    "    else:\n",
    "        print(\"ğŸ‰ ì™„ë£Œ! ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_annc = crawl_lh_notices_all_data()\n",
    "\n",
    "batch_id = lh_repo.bulk_insert_announcements(df_all_annc)\n",
    "\n",
    "\n",
    "print(batch_id)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
