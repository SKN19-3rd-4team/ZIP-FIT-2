{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e7d4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "time.sleep(1)  # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ í›„ ëŒ€ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffc2a353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v20251204.27.head+context\n"
     ]
    }
   ],
   "source": [
    "from zf_parser import ZipFitParser\n",
    "from crawl.lh import LH\n",
    "from database.repository import AnncLhRepository, AnncQrRepository, AnncAllRepository, AnncFileRepository, DocChunkRepository\n",
    "\n",
    "\n",
    "lh_repo = AnncLhRepository()\n",
    "qr_repo = AnncQrRepository()\n",
    "all_repo = AnncAllRepository()\n",
    "file_repo = AnncFileRepository()\n",
    "dc_repo = DocChunkRepository()\n",
    "\n",
    "\n",
    "lh_crwaler = LH()\n",
    "\n",
    "# ZipFitParser ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "zfp = ZipFitParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826857a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRWALING = True         # í¬ë¡¤ë§\n",
    "DB_BULK_INSERT = True   # í¬ë¡¤ë§ ë°ì´í„° DB INSERT\n",
    "TEST = False\n",
    "\n",
    "\n",
    "# batch_idê°€ ìˆìœ¼ë©´ í¬ë¡¤ë§, í¬ë¡¤ë§ ë°ì´í„° insert ì§„í–‰ ì•ˆí•¨\n",
    "batch_id = ''\n",
    "# batch_id = '6df7b622-2b10-4737-9ab6-2fba5f4214d1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a4fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_id:\n",
    "    CRWALING = False\n",
    "    DB_BULK_INSERT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dfcb513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "ğŸš€ LH ê³µê³  ë°ì´í„° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (POST ë°©ì‹)...\n",
      "**í•„í„° ê¸°ì¤€: [ìœ í˜• 'ì ‘ìˆ˜ì¤‘'] + ê²Œì‹œì¼ 2024-11-01 ì´í›„ ë°ì´í„° ìˆ˜ì§‘.**\n",
      "\n",
      "ğŸ“„ Crawling page 1 (0 to 1000)...\n",
      "âœ… í˜ì´ì§€ 1 ë¡œë“œ ì„±ê³µ. 9ê°œ ì¤‘ 9ê°œ ë°ì´í„° ì¶”ì¶œ.\n",
      "ğŸ í¬ë¡¤ë§ ì¢…ë£Œ ì¡°ê±´ ì¶©ì¡±. ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ.\n",
      "\n",
      "âœ¨ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ. ì´ 9ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n",
      "-----------------------------------------------------------------------------\n",
      "ğŸš€ LH ê³µê³  ë°ì´í„° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (POST ë°©ì‹)...\n",
      "**í•„í„° ê¸°ì¤€: [ìœ í˜• 'ê³µê³ ì¤‘'] + ê²Œì‹œì¼ 2024-11-01 ì´í›„ ë°ì´í„° ìˆ˜ì§‘.**\n",
      "\n",
      "ğŸ“„ Crawling page 1 (0 to 1000)...\n",
      "âœ… í˜ì´ì§€ 1 ë¡œë“œ ì„±ê³µ. 118ê°œ ì¤‘ 118ê°œ ë°ì´í„° ì¶”ì¶œ.\n",
      "ğŸ í¬ë¡¤ë§ ì¢…ë£Œ ì¡°ê±´ ì¶©ì¡±. ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ.\n",
      "\n",
      "âœ¨ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ. ì´ 118ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if CRWALING:\n",
    "    df_all_annc = lh_crwaler.crawl_lh_notices_all_data('ì ‘ìˆ˜ì¤‘')\n",
    "    df_all_annc += lh_crwaler.crawl_lh_notices_all_data('ê³µê³ ì¤‘')\n",
    "    # df_all_annc += lh_crwaler.crawl_lh_notices_all_data('ì •ì •ê³µê³ ì¤‘')\n",
    "else:\n",
    "    df_all_annc = []\n",
    "    print(\"í¬ë¡¤ë§ x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e9b7efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¤‘ë³µì œê±° 127 â¡ï¸ 118\n"
     ]
    }
   ],
   "source": [
    "len_before = len(df_all_annc)\n",
    "unique_tuples = set(tuple(sorted(d.items())) for d in df_all_annc)\n",
    "df_all_annc = [dict(t) for t in unique_tuples]\n",
    "len_after = len(df_all_annc)\n",
    "\n",
    "print(f'ì¤‘ë³µì œê±° {len_before} â¡ï¸ {len_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "700dc0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e3ecee61-b362-40ac-9fb2-5658d27d89f2\n"
     ]
    }
   ],
   "source": [
    "if DB_BULK_INSERT: \n",
    "    batch_id = lh_repo.bulk_insert_announcements(df_all_annc)\n",
    "else:\n",
    "    print(\"í¬ë¡¤ë§ ë°ì´í„° ì‚½ì… x\")\n",
    "\n",
    "\n",
    "print(batch_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cb8bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not batch_id:\n",
    "    raise \"batch_idê°€ ì—†ìœ¼ë©´ ì§„í–‰ ë¶ˆê°€.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da5f154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    select distinct alt.batch_id,alt.batch_seq,alt.annc_title,alt.annc_url,alt.batch_status,alt.batch_start_dttm,alt.annc_type,alt.annc_dtl_type,alt.annc_region,alt.annc_pblsh_dt,alt.annc_deadline_dt,alt.annc_status,alt.lh_pan_id,alt.lh_ais_tp_cd,alt.lh_upp_ais_tp_cd,alt.lh_ccr_cnnt_sys_ds_cd,alt.lh_ls_sst\n",
      "                    from (\n",
      "                        select alt.batch_id,alt.batch_seq,alt.annc_title,alt.annc_url,alt.batch_status,alt.batch_start_dttm,alt.annc_type,alt.annc_dtl_type,alt.annc_region,alt.annc_pblsh_dt,alt.annc_deadline_dt,alt.annc_status,alt.lh_pan_id,alt.lh_ais_tp_cd,alt.lh_upp_ais_tp_cd,alt.lh_ccr_cnnt_sys_ds_cd,alt.lh_ls_sst\n",
      "                        from annc_lh_temp alt\n",
      "                        where alt.batch_id = %s\n",
      "                            and not exists(\n",
      "                                select *\n",
      "                                from annc_all aa\n",
      "                                where aa.annc_url = alt.annc_url\n",
      "                            )\n",
      "                            and alt.annc_type in %s\n",
      "                            and (%s is null or alt.annc_status in %s)\n",
      "                        union all\n",
      "                        select alt.batch_id,alt.batch_seq,alt.annc_title,alt.annc_url,alt.batch_status,alt.batch_start_dttm,alt.annc_type,alt.annc_dtl_type,alt.annc_region,alt.annc_pblsh_dt,alt.annc_deadline_dt,alt.annc_status,alt.lh_pan_id,alt.lh_ais_tp_cd,alt.lh_upp_ais_tp_cd,alt.lh_ccr_cnnt_sys_ds_cd,alt.lh_ls_sst\n",
      "                        from annc_lh_temp alt\n",
      "                            join annc_all aa on alt.annc_url = aa.annc_url\n",
      "                        where alt.batch_id = %s\n",
      "                            and (alt.annc_pblsh_dt != aa.annc_pblsh_dt\n",
      "                            or alt.annc_deadline_dt != aa.annc_deadline_dt\n",
      "                            or alt.annc_status != aa.annc_status)\n",
      "                            and alt.annc_type in %s\n",
      "                            and (%s is null or alt.annc_status in %s)\n",
      "                            \n",
      "                    ) alt\n",
      "                    where batch_status not in ('COMPLETE')\n",
      "                    and annc_title in ('2025ë…„ ì‹ í˜¼Â·ì‹ ìƒì•„ ì „ì„¸ì„ëŒ€â…¡ ì…ì£¼ì ìˆ˜ì‹œëª¨ì§‘ ê³µê³ ','2025ë…„ ì‹ í˜¼Â·ì‹ ìƒì•„ ì „ì„¸ì„ëŒ€ I ì…ì£¼ì ìˆ˜ì‹œëª¨ì§‘ ê³µê³ ','2025ë…„ ì²­ë…„ ì „ì„¸ì„ëŒ€ 1ìˆœìœ„ ì…ì£¼ì ìˆ˜ì‹œëª¨ì§‘')\n",
      "                    order by alt.batch_seq asc;\n",
      "                \n",
      "1ê±´ ì¡°íšŒ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "annc_list_lh = qr_repo.get_announcements_merge_target(batch_id, annc_status=['ê³µê³ ì¤‘','ì ‘ìˆ˜ì¤‘'])\n",
    "\n",
    "print(f'{len(annc_list_lh)}ê±´ ì¡°íšŒ ì™„ë£Œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4876018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annc_list_lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c47e1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    annc_list_lh = annc_list_lh[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07a90316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annc_list_lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1af09976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6891b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "[00ë¶„00ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - ì‹œì‘! - 2025ë…„ ì²­ë…„ ì „ì„¸ì„ëŒ€ 1ìˆœìœ„ ì…ì£¼ì ìˆ˜ì‹œëª¨ì§‘\n",
      "[00ë¶„00ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - ê³µê³  ë‹«ê¸° ì²˜ë¦¬\n",
      "[00ë¶„00ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼ ì´ˆê¸°í™”\n",
      "[00ë¶„00ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼(ë‹¨ê±´) ì‹œì‘!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\anaconda3\\envs\\zf2_back\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'apply.lh.or.kr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00ë¶„00ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼(ë‹¨ê±´) ë‹¤ìš´ ì™„ë£Œ! âœ… (00ë¶„00ì´ˆ ì†Œìš”)\n",
      "[00ë¶„00ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼(ë‹¨ê±´) DBë“±ë¡ ì™„ë£Œ! âœ… (00ë¶„00ì´ˆ ì†Œìš”)\n",
      "[00ë¶„00ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼(ë‹¨ê±´) markdown ë³€í™˜ ì‹œì‘!\n",
      "Started parsing the file under job_id f7cd4f29-bf7e-479c-b71b-4b0ff5aa41be\n",
      "[00ë¶„15ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼(ë‹¨ê±´) markdown ë³€í™˜ ì™„ë£Œ! âœ… (00ë¶„14ì´ˆ ì†Œìš”)\n",
      "[00ë¶„15ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼(ë‹¨ê±´) ì„ë² ë”© ì‹œì‘!\n",
      "[00ë¶„49ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼(ë‹¨ê±´) ì„ë² ë”© ì™„ë£Œ! âœ… (00ë¶„34ì´ˆ ì†Œìš”)\n",
      "[00ë¶„50ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼(ë‹¨ê±´) ì²­í¬ DB ë“±ë¡ ì™„ë£Œ! âœ… (00ë¶„00ì´ˆ ì†Œìš”)\n",
      "[00ë¶„50ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - íŒŒì¼(ë‹¨ê±´) íŒŒì¼ ì‚­ì œ ì™„ë£Œ! âœ…\n",
      "[00ë¶„50ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) ì™„ë£Œ! âœ…\n",
      "[00ë¶„50ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - ê³µê³  ì—´ê¸° ì²˜ë¦¬\n",
      "[00ë¶„50ì´ˆ ê²½ê³¼] ë°°ì¹˜(1/1) - ì¢…ë£Œ! - 2025ë…„ ì²­ë…„ ì „ì„¸ì„ëŒ€ 1ìˆœìœ„ ì…ì£¼ì ìˆ˜ì‹œëª¨ì§‘\n"
     ]
    }
   ],
   "source": [
    "# íƒ€ì„ë©ìŠ¤ í•„ìš”\n",
    "\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "error_cnt = 0\n",
    "\n",
    "# ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "start_time = time.time()\n",
    "\n",
    "def get_elapsed_time():\n",
    "    \"\"\"ê²½ê³¼ ì‹œê°„ì„ [00ë¶„00ì´ˆ ê²½ê³¼] í˜•ì‹ìœ¼ë¡œ ë°˜í™˜\"\"\"\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    minutes = elapsed // 60\n",
    "    seconds = elapsed % 60\n",
    "    return f\"[{minutes:02d}ë¶„{seconds:02d}ì´ˆ ê²½ê³¼]\"\n",
    "\n",
    "def get_task_time(task_start):\n",
    "    \"\"\"ì‘ì—… ì†Œìš” ì‹œê°„ì„ (00ë¶„00ì´ˆ ì†Œìš”) í˜•ì‹ìœ¼ë¡œ ë°˜í™˜\"\"\"\n",
    "    elapsed = int(time.time() - task_start)\n",
    "    minutes = elapsed // 60\n",
    "    seconds = elapsed % 60\n",
    "    return f\"({minutes:02d}ë¶„{seconds:02d}ì´ˆ ì†Œìš”)\"\n",
    "\n",
    "# print(f\"ì‹œì‘ {len(annc_list_lh)}\")\n",
    "\n",
    "for idx_row, row_lh in enumerate(annc_list_lh):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        batch_msg = f\"ë°°ì¹˜({idx_row+1}/{len(annc_list_lh)})\"\n",
    "\n",
    "        # 1. ì„ì‹œ í…Œì´ë¸” ìƒíƒœ ë³€ê²½ -> ì‹œì‘\n",
    "        print(\"-\"*77)\n",
    "        lh_repo.update_announcements('START', row_lh['batch_id'], row_lh['batch_seq'])\n",
    "        print(f\"{get_elapsed_time()} {batch_msg} - ì‹œì‘! - {row_lh['annc_title']}\")\n",
    "        \n",
    "        # 2. ê³µê³  í…Œì´ë¸”ì— ë„£ì„ ë°ì´í„° ì¤€ë¹„\n",
    "        row_lh['corp_cd'] = 'LH'\n",
    "        row_lh['service_status'] = 'CLOSE'\n",
    "\n",
    "        merge_result = all_repo.merge_announcements([row_lh,]) # ì›ë˜ ë‹¤ê±´ì„ ìœ„í•œê²ƒ\n",
    "        print(f\"{get_elapsed_time()} {batch_msg} - ê³µê³  ë‹«ê¸° ì²˜ë¦¬\")\n",
    "        \n",
    "        if not merge_result:\n",
    "            raise Exception(\"ë¨¸ì§€ëœ í–‰ ì—†ìŒ\")\n",
    "        # 1í–‰ë§Œ ì“°ê² ìŒ\n",
    "        merge_result = merge_result[0]\n",
    "        annc_id = merge_result['annc_id']\n",
    "\n",
    "        # 3. íŒŒì¼ ì¡°íšŒ\n",
    "        file_list = lh_crwaler.get_file_list(row_lh)\n",
    "\n",
    "        if not file_list:\n",
    "            raise Exception(\"íŒŒì¼ ì—†ìŒ\")\n",
    "        \n",
    "        \n",
    "        # íŒŒì¼ ë°ì´í„° ì´ˆê¸°í™”\n",
    "        dc_repo.delete_chunks_by_annc_id(annc_id)\n",
    "        file_repo.delete_files_by_annc_id(annc_id)\n",
    "        print(f\"{get_elapsed_time()} {batch_msg} - íŒŒì¼ ì´ˆê¸°í™”\")\n",
    "        \n",
    "\n",
    "        # 4. íŒŒì¼ ë“±ë¡\n",
    "        for idx_file, file_info in enumerate(file_list):\n",
    "            if len(file_list) > 1:\n",
    "                file_msg = f\"íŒŒì¼({idx_file+1}/{len(file_list)})\"\n",
    "            else:\n",
    "                file_msg = f\"íŒŒì¼(ë‹¨ê±´)\"\n",
    "            print(f\"{get_elapsed_time()} {batch_msg} - {file_msg} ì‹œì‘!\")\n",
    "\n",
    "\n",
    "            annc_file = {}\n",
    "\n",
    "            annc_file['annc_id'] = annc_id\n",
    "            annc_file['file_name'] = file_info['cmnAhflNm']\n",
    "            annc_file['file_type'] = file_info['slPanAhflDsCdNm']\n",
    "            annc_file['file_ext'] = 'pdf'\n",
    "\n",
    "            # íŒŒì¼ ë‹¤ìš´\n",
    "            task_start = time.time()\n",
    "            file_path, annc_file = lh_crwaler.down_file(file_info['cmnAhflSn'], annc_file)\n",
    "            print(f\"{get_elapsed_time()} {batch_msg} - {file_msg} ë‹¤ìš´ ì™„ë£Œ! âœ… {get_task_time(task_start)}\")\n",
    "\n",
    "            # íŒŒì¼ ë“±ë¡\n",
    "            task_start = time.time()\n",
    "            inserted_file_info = file_repo.bulk_insert_files([annc_file])[0]\n",
    "            print(f\"{get_elapsed_time()} {batch_msg} - {file_msg} DBë“±ë¡ ì™„ë£Œ! âœ… {get_task_time(task_start)}\")\n",
    "\n",
    "            file_id, file_name = inserted_file_info['file_id'], inserted_file_info['file_name']\n",
    "\n",
    "            # pdf -> markdown\n",
    "            print(f\"{get_elapsed_time()} {batch_msg} - {file_msg} markdown ë³€í™˜ ì‹œì‘!\")\n",
    "            task_start = time.time()\n",
    "            elements = zfp.get_llama_parsed_docs(file_path)\n",
    "            print(f\"{get_elapsed_time()} {batch_msg} - {file_msg} markdown ë³€í™˜ ì™„ë£Œ! âœ… {get_task_time(task_start)}\")\n",
    "\n",
    "            print(f\"{get_elapsed_time()} {batch_msg} - {file_msg} ì„ë² ë”© ì‹œì‘!\")\n",
    "            task_start = time.time()\n",
    "                        \n",
    "            chunk_dto = [{\n",
    "                'file_id': file_id,\n",
    "                'annc_id': annc_id,\n",
    "                'chunk_type': el.get('element_type','text'),\n",
    "                'chunk_text': el.get('origin_content',''),\n",
    "                'page_num': el.get('page_number', 0),\n",
    "                'embedding': get_embedding(el.get('content', '')),\n",
    "                'metadata': json.dumps(el.get('metadata', {}))  # dictë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "            } for el in elements]\n",
    "                        \n",
    "            print(f\"{get_elapsed_time()} {batch_msg} - {file_msg} ì„ë² ë”© ì™„ë£Œ! âœ… {get_task_time(task_start)}\")\n",
    "\n",
    "            # ì²­í¬ ì‚½ì…\n",
    "            task_start = time.time()\n",
    "            dc_repo.bulk_insert_chunks(chunk_dto)\n",
    "            print(f\"{get_elapsed_time()} {batch_msg} - {file_msg} ì²­í¬ DB ë“±ë¡ ì™„ë£Œ! âœ… {get_task_time(task_start)}\")\n",
    "\n",
    "            # ì‚¬ìš©í•œ íŒŒì¼ ì‚­ì œ\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"{get_elapsed_time()} {batch_msg} - {file_msg} íŒŒì¼ ì‚­ì œ ì™„ë£Œ! âœ…\")\n",
    "\n",
    "        lh_repo.update_announcements('COMPLETE', row_lh['batch_id'], row_lh['batch_seq'])\n",
    "        print(f\"{get_elapsed_time()} {batch_msg} ì™„ë£Œ! âœ…\")\n",
    "\n",
    "        \n",
    "        row_lh['service_status'] = 'OPEN'\n",
    "        merge_result = all_repo.merge_announcements([row_lh,]) # ì›ë˜ ë‹¤ê±´ì„ ìœ„í•œê²ƒ\n",
    "        print(f\"{get_elapsed_time()} {batch_msg} - ê³µê³  ì—´ê¸° ì²˜ë¦¬\")\n",
    "        print(f\"{get_elapsed_time()} {batch_msg} - ì¢…ë£Œ! - {row_lh['annc_title']}\")\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        error_cnt += 1\n",
    "        \n",
    "        lh_repo.update_announcements('ERROR', row_lh['batch_id'], row_lh['batch_seq'])\n",
    "        print(f\"{get_elapsed_time()} {batch_msg} ì˜¤ë¥˜! âŒ - {e}\")\n",
    "\n",
    "        if error_cnt < 5:\n",
    "            continue\n",
    "\n",
    "        print(f\"{get_elapsed_time()} {batch_msg} í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ - ì˜¤ë¥˜ ì—¬ëŸ¬ê±´ ë°œê²¬\")\n",
    "        raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zf2_back",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
