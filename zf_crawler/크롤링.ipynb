{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e921818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.crawler import Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a13fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRWALING = True         # í¬ë¡¤ë§\n",
    "DB_BULK_INSERT = True   # í¬ë¡¤ë§ ë°ì´í„° DB INSERT\n",
    "\n",
    "TEST_SIZE = 1           # í¬ë¡¤ë§ ê³µê³  í…ŒìŠ¤íŠ¸ìš© ì‚¬ì´íŠ¸ ì„¤ì •ë˜ì–´ ìˆëŠ” ë§Œí¼ ì§„í–‰ë¨ ê¸°ë³¸ê°’ -1 = ì „ì²´ ì§„í–‰\n",
    "batch_id = ''           # ì¤‘ê°„ ì§„ì…ìœ„í•œ BATCH_ID ë„£ìœ¼ë©´ \n",
    "\n",
    "if batch_id:\n",
    "    CRWALING = False\n",
    "    DB_BULK_INSERT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ dotenv ëª¨ë“ˆ ì„¤ì¹˜ ì¤‘...\n",
      "âœ… dotenv ëª¨ë“ˆ ì„¤ì¹˜ ì™„ë£Œ\n",
      "âœ… .env íŒŒì¼ ë°œê²¬: c:\\SKN_19\\ZIP-FIT-2\\.env\n",
      "í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: c:\\SKN_19\\ZIP-FIT-2\\zf_crawler\n",
      "í”„ë¡œì íŠ¸ ë£¨íŠ¸: c:\\SKN_19\\ZIP-FIT-2\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° í•„ìˆ˜ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜\n",
    "# ============================================\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ëª©ë¡ (ë²„ì „ ì§€ì • ì—†ì´ ì„¤ì¹˜)\n",
    "REQUIRED_PACKAGES = [\n",
    "    'python-dotenv',\n",
    "    'beautifulsoup4',  # bs4\n",
    "    'requests',\n",
    "    'psycopg2-binary',\n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'llama-cloud-services',  # LlamaParse ì‚¬ìš©\n",
    "    'langchain-openai',  # ì„ë² ë”© ì‚¬ìš©\n",
    "]\n",
    "\n",
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•¨ìˆ˜ (ì˜¤ë¥˜ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰)\n",
    "def install_package(package_name):\n",
    "    \"\"\"íŒ¨í‚¤ì§€ê°€ ì—†ìœ¼ë©´ ì„¤ì¹˜ (ì˜¤ë¥˜ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰)\"\"\"\n",
    "    try:\n",
    "        # íŒ¨í‚¤ì§€ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "        if package_name == 'beautifulsoup4':\n",
    "            import bs4\n",
    "            print(f\"âœ… {package_name} (bs4) ì‚¬ìš© ê°€ëŠ¥\")\n",
    "            return True\n",
    "        elif package_name == 'python-dotenv':\n",
    "            import dotenv\n",
    "            print(f\"âœ… {package_name} ì‚¬ìš© ê°€ëŠ¥\")\n",
    "            return True\n",
    "        elif package_name == 'psycopg2-binary':\n",
    "            import psycopg2\n",
    "            print(f\"âœ… {package_name} (psycopg2) ì‚¬ìš© ê°€ëŠ¥\")\n",
    "            return True\n",
    "        elif package_name == 'llama-cloud-services':\n",
    "            import llama_cloud_services\n",
    "            print(f\"âœ… {package_name} ì‚¬ìš© ê°€ëŠ¥\")\n",
    "            return True\n",
    "        elif package_name == 'langchain-openai':\n",
    "            import langchain_openai\n",
    "            print(f\"âœ… {package_name} ì‚¬ìš© ê°€ëŠ¥\")\n",
    "            return True\n",
    "        else:\n",
    "            module_name = package_name.replace('-', '_')\n",
    "            __import__(module_name)\n",
    "            print(f\"âœ… {package_name} ì‚¬ìš© ê°€ëŠ¥\")\n",
    "            return True\n",
    "    except ImportError:\n",
    "        # íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹œë„\n",
    "        print(f\"ğŸ“¦ {package_name} ì„¤ì¹˜ ì‹œë„ ì¤‘...\")\n",
    "        try:\n",
    "            # ë²„ì „ ì¶©ëŒì„ í”¼í•˜ê¸° ìœ„í•´ --no-deps ì˜µì…˜ ì—†ì´ ì„¤ì¹˜\n",
    "            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰\n",
    "            result = subprocess.run(\n",
    "                [sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--quiet\"],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                print(f\"âœ… {package_name} ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"âš ï¸ {package_name} ì„¤ì¹˜ ì‹¤íŒ¨ (ì˜ì¡´ì„± ì¶©ëŒ ê°€ëŠ¥)\")\n",
    "                print(f\"   ìˆ˜ë™ ì„¤ì¹˜ í•„ìš”: pip install {package_name}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ {package_name} ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜: {str(e)[:50]}\")\n",
    "            return False\n",
    "\n",
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸ ë° ì„¤ì¹˜\n",
    "print(\"=\" * 50)\n",
    "print(\"í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸ ì¤‘...\")\n",
    "print(\"=\" * 50)\n",
    "installed_count = 0\n",
    "for package in REQUIRED_PACKAGES:\n",
    "    if install_package(package):\n",
    "        installed_count += 1\n",
    "print(\"=\" * 50)\n",
    "print(f\"íŒ¨í‚¤ì§€ í™•ì¸ ì™„ë£Œ: {installed_count}/{len(REQUIRED_PACKAGES)}\")\n",
    "\n",
    "# pgvectorëŠ” ë³„ë„ë¡œ ì²˜ë¦¬ (ì˜ì¡´ì„± ì¶©ëŒ ê°€ëŠ¥ì„± ë†’ìŒ)\n",
    "try:\n",
    "    import pgvector\n",
    "    print(\"âœ… pgvector ì‚¬ìš© ê°€ëŠ¥\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ pgvector ì—†ìŒ (í•„ìš”ì‹œ ìˆ˜ë™ ì„¤ì¹˜: pip install pgvector)\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# dotenv ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ ìœ„ì¹˜ í™•ì¸ (í”„ë¡œì íŠ¸ ë£¨íŠ¸)\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'zf_crawler' else Path.cwd()\n",
    "env_file = project_root / '.env'\n",
    "\n",
    "if env_file.exists():\n",
    "    print(f\"âœ… .env íŒŒì¼ ë°œê²¬: {env_file}\")\n",
    "    load_dotenv(env_file)\n",
    "else:\n",
    "    print(f\"âš ï¸ .env íŒŒì¼ ì—†ìŒ: {env_file}\")\n",
    "    print(\"í™˜ê²½ ë³€ìˆ˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œë©ë‹ˆë‹¤.\")\n",
    "    load_dotenv()  # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œë„ ì°¾ìŒ\n",
    "\n",
    "print(f\"\\ní˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "print(f\"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d5a21",
   "metadata": {},
   "source": [
    "## 1. í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc85114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annc_all: âœ…\n",
      "annc_files: âœ…\n",
      "annc_lh_temp: âœ…\n",
      "doc_chunks: âœ…\n"
     ]
    }
   ],
   "source": [
    "from src.database import DataBaseHandler\n",
    "\n",
    "tables = (\"annc_all\",\"annc_files\",\"annc_lh_temp\",\"doc_chunks\")\n",
    "\n",
    "cases = ','.join([f\"\"\"sum(case when table_name in ('{table}') then 1 else 0 end) as {table}\"\"\" for table in tables])\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "{cases}\n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = 'public'\n",
    "AND table_name = any(%s)\n",
    "\"\"\"\n",
    "\n",
    "with DataBaseHandler() as db:\n",
    "    result = db.execute_query(query, (list(tables),))\n",
    "\n",
    "zero_count = 0\n",
    "for val in result[0].items():\n",
    "    table_name = val[0]\n",
    "    table_count = val[1]\n",
    "    print(f\"{val[0]}: {'âœ…' if val[1] > 0 else 'âŒ'}\")\n",
    "    if table_count == 0:\n",
    "        zero_count += 1\n",
    "if zero_count > 0:\n",
    "    print(\"í…Œì´ë¸” ì´ìƒ ìƒíƒœ í™•ì¸ í•„ìš”\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57193d",
   "metadata": {},
   "source": [
    "## 2. í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c13f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "ğŸš€ LH ê³µê³  ë°ì´í„° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (POST ë°©ì‹)...\n",
      "**í•„í„° ê¸°ì¤€: [ìœ í˜• 'ì ‘ìˆ˜ì¤‘'] + ê²Œì‹œì¼ 2024-11-01 ì´í›„ ë°ì´í„° ìˆ˜ì§‘.**\n",
      "\n",
      "ğŸ“„ Crawling page 1 (0 to 1000)...\n",
      "âœ… í˜ì´ì§€ 1 ë¡œë“œ ì„±ê³µ. 15ê°œ ì¤‘ 15ê°œ ë°ì´í„° ì¶”ì¶œ.\n",
      "ğŸ í¬ë¡¤ë§ ì¢…ë£Œ ì¡°ê±´ ì¶©ì¡±. ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ.\n",
      "\n",
      "âœ¨ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ. ì´ 15ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n",
      "-----------------------------------------------------------------------------\n",
      "ğŸš€ LH ê³µê³  ë°ì´í„° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (POST ë°©ì‹)...\n",
      "**í•„í„° ê¸°ì¤€: [ìœ í˜• 'ê³µê³ ì¤‘'] + ê²Œì‹œì¼ 2024-11-01 ì´í›„ ë°ì´í„° ìˆ˜ì§‘.**\n",
      "\n",
      "ğŸ“„ Crawling page 1 (0 to 1000)...\n",
      "âœ… í˜ì´ì§€ 1 ë¡œë“œ ì„±ê³µ. 129ê°œ ì¤‘ 129ê°œ ë°ì´í„° ì¶”ì¶œ.\n",
      "ğŸ í¬ë¡¤ë§ ì¢…ë£Œ ì¡°ê±´ ì¶©ì¡±. ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ.\n",
      "\n",
      "âœ¨ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ. ì´ 129ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í¬ë¡¤ë§ ë³¸ ë¡œì§\n",
    "\n",
    "from src.crawler.lh import LH\n",
    "\n",
    "\n",
    "lh_crwaler = LH()\n",
    "\n",
    "if CRWALING:\n",
    "    df_all_annc = lh_crwaler.crawl_lh_notices_all_data('ì ‘ìˆ˜ì¤‘')\n",
    "    df_all_annc += lh_crwaler.crawl_lh_notices_all_data('ê³µê³ ì¤‘')\n",
    "    # df_all_annc += lh_crwaler.crawl_lh_notices_all_data('ì •ì •ê³µê³ ì¤‘')\n",
    "else:\n",
    "    df_all_annc = []\n",
    "    print(\"í¬ë¡¤ë§ x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1468ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¤‘ë³µì œê±° 144 â¡ï¸ 129\n"
     ]
    }
   ],
   "source": [
    "# í¬ë¡¤ë§ ëœ ë°ì´í„° ì¤‘ë³µì œê±°\n",
    "\n",
    "len_before = len(df_all_annc)\n",
    "unique_tuples = set(tuple(sorted(d.items())) for d in df_all_annc)\n",
    "df_all_annc = [dict(t) for t in unique_tuples]\n",
    "len_after = len(df_all_annc)\n",
    "\n",
    "print(f'ì¤‘ë³µì œê±° {len_before} â¡ï¸ {len_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c83fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°íƒ€í•­ëª© ì œê±° 129 â¡ï¸ 37\n"
     ]
    }
   ],
   "source": [
    "len_before = len(df_all_annc)\n",
    "df_all_annc = [annc for annc in df_all_annc if annc['annc_type'] in ('ë§¤ì…','ì„ëŒ€')]\n",
    "len_after = len(df_all_annc)\n",
    "\n",
    "print(f'ê¸°íƒ€í•­ëª© ì œê±° {len_before} â¡ï¸ {len_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67814d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìˆ˜ë„ê¶Œ í•œì • 37 â¡ï¸ 3\n"
     ]
    }
   ],
   "source": [
    "len_before = len(df_all_annc)\n",
    "df_all_annc = [annc for annc in df_all_annc if annc['annc_region'] in ('ì„œìš¸íŠ¹ë³„ì‹œ','ê²½ê¸°ë„')]\n",
    "len_after = len(df_all_annc)\n",
    "\n",
    "print(f'ìˆ˜ë„ê¶Œ í•œì • {len_before} â¡ï¸ {len_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e22352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'annc_deadline_dt': '2025.12.19',\n",
       "  'annc_dtl_type': 'ì˜êµ¬ì„ëŒ€',\n",
       "  'annc_pblsh_dt': '2025.11.27',\n",
       "  'annc_region': 'ê²½ê¸°ë„',\n",
       "  'annc_status': 'ì •ì •ê³µê³ ì¤‘',\n",
       "  'annc_title': 'ì–‘ì£¼íšŒì²œ A25BL ì˜êµ¬ì„ëŒ€ì£¼íƒ ì…ì£¼ì ëª¨ì§‘ê³µê³ ',\n",
       "  'annc_type': 'ì„ëŒ€',\n",
       "  'annc_url': 'https://apply.lh.or.kr/lhapply/apply/wt/wrtanc/selectWrtancInfo.do?mi=1026&panId=2015122300019102&ccrCnntSysDsCd=03&uppAisTpCd=06&aisTpCd=09',\n",
       "  'lh_ais_tp_cd': '09',\n",
       "  'lh_ccr_cnnt_sys_ds_cd': '03',\n",
       "  'lh_ls_sst': '',\n",
       "  'lh_pan_id': '2015122300019102',\n",
       "  'lh_upp_ais_tp_cd': '06'},\n",
       " {'annc_deadline_dt': '2025.12.31',\n",
       "  'annc_dtl_type': 'í–‰ë³µì£¼íƒ',\n",
       "  'annc_pblsh_dt': '2025.05.28',\n",
       "  'annc_region': 'ê²½ê¸°ë„',\n",
       "  'annc_status': 'ê³µê³ ì¤‘',\n",
       "  'annc_title': 'ì–‘ì£¼ì‹œ, ë™ë‘ì²œì‹œ í–‰ë³µì£¼íƒ ìƒì‹œëª¨ì§‘[ì„ ì°©ìˆœë™í˜¸ì§€ì •, ì…ì£¼ìê²©ì™„í™”, ì„ ê³„ì•½í›„ê²€ì¦]',\n",
       "  'annc_type': 'ì„ëŒ€',\n",
       "  'annc_url': 'https://apply.lh.or.kr/lhapply/apply/wt/wrtanc/selectWrtancInfo.do?mi=1026&panId=2015122300018161&ccrCnntSysDsCd=03&uppAisTpCd=06&aisTpCd=10',\n",
       "  'lh_ais_tp_cd': '10',\n",
       "  'lh_ccr_cnnt_sys_ds_cd': '03',\n",
       "  'lh_ls_sst': '',\n",
       "  'lh_pan_id': '2015122300018161',\n",
       "  'lh_upp_ais_tp_cd': '06'},\n",
       " {'annc_deadline_dt': '2025.12.19',\n",
       "  'annc_dtl_type': 'ì˜êµ¬ì„ëŒ€',\n",
       "  'annc_pblsh_dt': '2025.12.02',\n",
       "  'annc_region': 'ê²½ê¸°ë„',\n",
       "  'annc_status': 'ê³µê³ ì¤‘',\n",
       "  'annc_title': '[ì •ì •ê³µê³ ]ì–‘ì£¼íšŒì²œ A25BL ì˜êµ¬ì„ëŒ€ì£¼íƒ ì…ì£¼ì ëª¨ì§‘ê³µê³ ',\n",
       "  'annc_type': 'ì„ëŒ€',\n",
       "  'annc_url': 'https://apply.lh.or.kr/lhapply/apply/wt/wrtanc/selectWrtancInfo.do?mi=1026&panId=2015122300019125&ccrCnntSysDsCd=03&uppAisTpCd=06&aisTpCd=09',\n",
       "  'lh_ais_tp_cd': '09',\n",
       "  'lh_ccr_cnnt_sys_ds_cd': '03',\n",
       "  'lh_ls_sst': '',\n",
       "  'lh_pan_id': '2015122300019125',\n",
       "  'lh_upp_ais_tp_cd': '06'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_annc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedde30",
   "metadata": {},
   "source": [
    "# 3. ëŒ€ìƒ DB INSERT - BATCH ID ë°œí–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f33f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.database.repository import (\n",
    "    AnncLhRepository, AnncQrRepository,\n",
    "    AnncAllRepository, AnncFileRepository,\n",
    "    DocChunkRepository,\n",
    ")\n",
    "\n",
    "lh_repo = AnncLhRepository()\n",
    "# qr_repo = AnncQrRepository()\n",
    "all_repo = AnncAllRepository()\n",
    "file_repo = AnncFileRepository()\n",
    "dc_repo = DocChunkRepository()\n",
    "\n",
    "if DB_BULK_INSERT: \n",
    "    batch_id = lh_repo.bulk_insert_announcements(df_all_annc)\n",
    "else:\n",
    "    print(\"í¬ë¡¤ë§ ë°ì´í„° ì‚½ì… x\")\n",
    "\n",
    "if not batch_id:\n",
    "    raise \"batch_idê°€ ì—†ìœ¼ë©´ ì§„í–‰ ë¶ˆê°€.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c523a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2ê±´ ì¡°íšŒ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "annc_list_lh = AnncQrRepository().get_announcements_merge_target(batch_id, annc_status=['ê³µê³ ì¤‘','ì ‘ìˆ˜ì¤‘'])\n",
    "\n",
    "print(f'{len(annc_list_lh)}ê±´ ì¡°íšŒ ì™„ë£Œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e88f5e",
   "metadata": {},
   "source": [
    "# 4. íŒŒì¼ ë“±ë¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb19f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from src.parser import parse_pdf\n",
    "from src.chunker import create_chunks_from_elements\n",
    "from src.embedder import embed_chunks\n",
    "import json\n",
    "\n",
    "def process(row_lh, corp_cd):\n",
    "    \"\"\"\n",
    "    ê³µê³  ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    time_laps = []\n",
    "\n",
    "    title_now = lambda title: (title, datetime.now())\n",
    "\n",
    "    # 1. ë°°ì¹˜ í…Œì´ë¸” ìƒíƒœ ë³€ê²½ -> ì‹œì‘\n",
    "    lh_repo.update_announcements('START', row_lh['batch_id'], row_lh['batch_seq'])\n",
    "    time_laps.append(title_now(f\"ë°°ì¹˜ ì‹œì‘ - {row_lh['annc_title']}\"))\n",
    "\n",
    "    # 2. ê³µê³  ë‹«ê¸° ì²˜ë¦¬\n",
    "    row_lh['corp_cd'] = corp_cd\n",
    "    row_lh['service_status'] = 'CLOSE'\n",
    "    row_lh['created_dttm'] = datetime.now()\n",
    "    row_lh['updated_dttm'] = datetime.now()\n",
    "    merge_result = all_repo.merge_announcements([row_lh,]) # ì›ë˜ ë‹¤ê±´ì„ ìœ„í•œê²ƒ\n",
    "    time_laps.append(title_now(f\"ê³µê³  ë‹«ê¸° ì²˜ë¦¬\"))\n",
    "\n",
    "    if not merge_result:\n",
    "        raise Exception(\"ë¨¸ì§€ëœ í–‰ ì—†ìŒ\")\n",
    "    \n",
    "    merge_result = merge_result[0]\n",
    "    annc_id = merge_result['annc_id']\n",
    "\n",
    "    # 3. íŒŒì¼ ì¡°íšŒ\n",
    "    file_list = lh_crwaler.get_file_list(row_lh)\n",
    "    time_laps.append(title_now(f\"íŒŒì¼ ì¡°íšŒ\"))\n",
    "\n",
    "    if not file_list:\n",
    "        raise Exception(\"íŒŒì¼ ì—†ìŒ\")\n",
    "    \n",
    "    # 4. íŒŒì¼, ì²­í¬ ë²¡í„° ì´ˆê¸°í™”\n",
    "    dc_repo.delete_chunks_by_annc_id(annc_id)\n",
    "    file_repo.delete_files_by_annc_id(annc_id)\n",
    "    # print(f\"{get_elapsed_time()} {batch_msg} - íŒŒì¼ ì´ˆê¸°í™”\")\n",
    "    time_laps.append(title_now(f\"íŒŒì¼ ê¸°ë¡ ì‚­ì œ ì²˜ë¦¬\"))\n",
    "\n",
    "    for idx_file, file_info in enumerate(file_list):\n",
    "        time_laps.append(title_now(f\"íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ({idx_file+1}/{len(file_list)})\"))\n",
    "\n",
    "        annc_file = {}\n",
    "\n",
    "        annc_file['annc_id'] = annc_id\n",
    "        annc_file['file_name'] = file_info['cmnAhflNm']\n",
    "        annc_file['file_type'] = file_info['slPanAhflDsCdNm']\n",
    "        annc_file['file_ext'] = 'pdf'\n",
    "\n",
    "        # íŒŒì¼ ë‹¤ìš´\n",
    "        file_path, annc_file = lh_crwaler.down_file(file_info['cmnAhflSn'], annc_file)\n",
    "        time_laps.append(title_now(f\"íŒŒì¼ ë‹¤ìš´ë¡œë“œ\"))\n",
    "\n",
    "        # íŒŒì¼ DB ë“±ë¡\n",
    "        inserted_file_info = file_repo.bulk_insert_files([annc_file])[0]\n",
    "        file_id, file_name = inserted_file_info['file_id'], inserted_file_info['file_name']\n",
    "        time_laps.append(title_now(f\"íŒŒì¼ ì •ë³´ DB ê¸°ë¡\"))\n",
    "\n",
    "        # íŒŒì¼ ì—˜ë¦¬ë¨¼íŠ¸ êµ¬ì„±\n",
    "        parsed = parse_pdf(file_path, annc_id)\n",
    "        time_laps.append(title_now(f\"íŒŒì¼ -> Markdown\"))\n",
    "        # test_elements = parsed.elements[:10]\n",
    "\n",
    "        chunks = create_chunks_from_elements(parsed.elements, annc_id)\n",
    "        time_laps.append(title_now(f\"Markdown -> ì²­í¬\"))\n",
    "\n",
    "        embed_chunks(chunks)\n",
    "        time_laps.append(title_now(f\"ì²­í¬ -> ì„ë² ë”©\"))\n",
    "\n",
    "        chunk_dto = [{\n",
    "            'file_id': file_id,\n",
    "            'annc_id': annc_id,\n",
    "            'chunk_type': c.element_type,#get('element_type','text'),\n",
    "            'chunk_text': c.text, #get('text',''),\n",
    "            'page_num': c.page_number,\n",
    "            'embedding': c.embedding,\n",
    "            'metadata': json.dumps(c.metadata)  # dictë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "        } for c in chunks]\n",
    "\n",
    "        \n",
    "        dc_repo.bulk_insert_chunks(chunk_dto)\n",
    "        time_laps.append(title_now(f\"ì„ë² ë”© ì •ë³´ ê¸°ë¡\"))\n",
    "\n",
    "        # ì‚¬ìš©í•œ íŒŒì¼ ì‚­ì œ\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "        time_laps.append(title_now(f\"íŒŒì¼ ì‚­ì œ\"))\n",
    "\n",
    "        # return chunks\n",
    "    lh_repo.update_announcements('COMPLETE', row_lh['batch_id'], row_lh['batch_seq'])\n",
    "    time_laps.append(title_now(f\"ë°°ì¹˜ ì¢…ë£Œ\"))\n",
    "\n",
    "    # print(parsed)\n",
    "    row_lh['service_status'] = 'OPEN'\n",
    "    merge_result = all_repo.merge_announcements([row_lh,]) # ì›ë˜ ë‹¤ê±´ì„ ìœ„í•œê²ƒ\n",
    "    time_laps.append(title_now(f\"ë°°ì¹˜ ì™„ë£Œ ì²˜ë¦¬\"))\n",
    "\n",
    "    return time_laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "078183bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time_laps(time_laps):\n",
    "    \"\"\"\n",
    "    process()ì˜ ë°˜í™˜ê°’(time_laps)ì„ ë°›ì•„\n",
    "    - ì§ì „ ë‹¨ê³„ ëŒ€ë¹„ ì†Œìš”ì‹œê°„\n",
    "    - ì‹œì‘ ì´í›„ ì´ ì†Œìš”ì‹œê°„\n",
    "    ì„ í•¨ê»˜ ì¶œë ¥í•œë‹¤.\n",
    "    \"\"\"\n",
    "    if not time_laps:\n",
    "        print(\"ì‹œê°„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    start_time = time_laps[0][1]\n",
    "\n",
    "    for i, (title, t) in enumerate(time_laps):\n",
    "        if i == 0:\n",
    "            diff_prev = timedelta(0)\n",
    "        else:\n",
    "            diff_prev = t - time_laps[i-1][1]\n",
    "\n",
    "        diff_total = t - start_time\n",
    "\n",
    "        print(f\"[{i+1:02d}] {title}\")\n",
    "        print(f\"  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: {diff_prev}\")\n",
    "        print(f\"  â”” ì „ì²´ ê²½ê³¼   : {diff_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1639a",
   "metadata": {},
   "source": [
    "## ì‹¤í–‰ ë° íƒ€ì„ ë©ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3d1110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\anaconda3\\envs\\zipfit_env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'apply.lh.or.kr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 7a2291f5-314a-4517-9e3f-cc3784f249e5\n",
      ".[01] ë°°ì¹˜ ì‹œì‘ - ì–‘ì£¼ì‹œ, ë™ë‘ì²œì‹œ í–‰ë³µì£¼íƒ ìƒì‹œëª¨ì§‘[ì„ ì°©ìˆœë™í˜¸ì§€ì •, ì…ì£¼ìê²©ì™„í™”, ì„ ê³„ì•½í›„ê²€ì¦]\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00\n",
      "[02] ê³µê³  ë‹«ê¸° ì²˜ë¦¬\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.044041\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.044041\n",
      "[03] íŒŒì¼ ì¡°íšŒ\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.129632\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.173673\n",
      "[04] íŒŒì¼ ê¸°ë¡ ì‚­ì œ ì²˜ë¦¬\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.064013\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.237686\n",
      "[05] íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ (1/1)\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.237686\n",
      "[06] íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.337277\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.574963\n",
      "[07] íŒŒì¼ ì •ë³´ DB ê¸°ë¡\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.046813\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.621776\n",
      "[08] íŒŒì¼ -> Markdown\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:01:47.591553\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:48.213329\n",
      "[09] Markdown -> ì²­í¬\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.011290\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:48.224619\n",
      "[10] ì²­í¬ -> ì„ë² ë”©\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:05.427554\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:53.652173\n",
      "[11] ì„ë² ë”© ì •ë³´ ê¸°ë¡\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.605051\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:54.257224\n",
      "[12] íŒŒì¼ ì‚­ì œ\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:54.257224\n",
      "[13] ë°°ì¹˜ ì¢…ë£Œ\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.017698\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:54.274922\n",
      "[14] ë°°ì¹˜ ì™„ë£Œ ì²˜ë¦¬\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.017265\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:54.292187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\anaconda3\\envs\\zipfit_env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'apply.lh.or.kr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 40cf431c-8a73-4466-9d52-8f71a5d9c626\n",
      ".[01] ë°°ì¹˜ ì‹œì‘ - [ì •ì •ê³µê³ ]ì–‘ì£¼íšŒì²œ A25BL ì˜êµ¬ì„ëŒ€ì£¼íƒ ì…ì£¼ì ëª¨ì§‘ê³µê³ \n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00\n",
      "[02] ê³µê³  ë‹«ê¸° ì²˜ë¦¬\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.018709\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.018709\n",
      "[03] íŒŒì¼ ì¡°íšŒ\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.103225\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.121934\n",
      "[04] íŒŒì¼ ê¸°ë¡ ì‚­ì œ ì²˜ë¦¬\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.050962\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.172896\n",
      "[05] íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ (1/1)\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.172896\n",
      "[06] íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.352924\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.525820\n",
      "[07] íŒŒì¼ ì •ë³´ DB ê¸°ë¡\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.046482\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:00:00.572302\n",
      "[08] íŒŒì¼ -> Markdown\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:01:48.066881\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:48.639183\n",
      "[09] Markdown -> ì²­í¬\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.011675\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:48.650858\n",
      "[10] ì²­í¬ -> ì„ë² ë”©\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:02.488688\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:51.139546\n",
      "[11] ì„ë² ë”© ì •ë³´ ê¸°ë¡\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.702272\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:51.841818\n",
      "[12] íŒŒì¼ ì‚­ì œ\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:51.841818\n",
      "[13] ë°°ì¹˜ ì¢…ë£Œ\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.031259\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:51.873077\n",
      "[14] ë°°ì¹˜ ì™„ë£Œ ì²˜ë¦¬\n",
      "  â”œ ì „ ë‹¨ê³„ ëŒ€ë¹„: 0:00:00.017056\n",
      "  â”” ì „ì²´ ê²½ê³¼   : 0:01:51.890133\n"
     ]
    }
   ],
   "source": [
    "for row in annc_list_lh:\n",
    "    time_laps = process(row,\"LH\")\n",
    "\n",
    "    print_time_laps(time_laps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zipfit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
